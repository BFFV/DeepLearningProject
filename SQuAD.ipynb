{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "SQuAD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.1 64-bit ('venv')"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "182a1546f291458ebd9af05460879505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fcc84ae23ac84584a99ca37c69fa1e60",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4866f7da882045fab3693d29c7a91f1c",
              "IPY_MODEL_e207cd6e8f4642588cdfe2b1c0a65408"
            ]
          }
        },
        "fcc84ae23ac84584a99ca37c69fa1e60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4866f7da882045fab3693d29c7a91f1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0189793570c04068af4f25846fc0b54f",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1869,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1869,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0a23ba379ae47c385d80f7b7a83ff98"
          }
        },
        "e207cd6e8f4642588cdfe2b1c0a65408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7cf50f06c9df4faba683e88754fc1957",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.28k/? [00:02&lt;00:00, 1.78kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_65ccddd696db4003aa6b2adcda8d168c"
          }
        },
        "0189793570c04068af4f25846fc0b54f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0a23ba379ae47c385d80f7b7a83ff98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7cf50f06c9df4faba683e88754fc1957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "65ccddd696db4003aa6b2adcda8d168c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1d8e0b3e4318469d9641c2bd0980109a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d47075d6fd8248b79e766dde90151ebe",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bf042832eed5449eba01b90db0e27972",
              "IPY_MODEL_c579bc187fcd4d3a922d75e936e7a089"
            ]
          }
        },
        "d47075d6fd8248b79e766dde90151ebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf042832eed5449eba01b90db0e27972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e70b8e179ad148e59e73a435d57b2863",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1024,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1024,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2a0a72a5ec54b979849e9a87be8c5d5"
          }
        },
        "c579bc187fcd4d3a922d75e936e7a089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a034558e746b4abe89c6471cb4704e8a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.40k/? [00:02&lt;00:00, 899B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8480a978d1ad498f88e33ea40bb720f0"
          }
        },
        "e70b8e179ad148e59e73a435d57b2863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2a0a72a5ec54b979849e9a87be8c5d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a034558e746b4abe89c6471cb4704e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8480a978d1ad498f88e33ea40bb720f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3233eb5925ef41d1838670755ef7010f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2f9cfa75e97b4d309c70543d17bcb458",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f5cb57dddd6f4699a0214ff9cf59db9f",
              "IPY_MODEL_40a6209290a94ea1a3f89955a83a0563"
            ]
          }
        },
        "2f9cfa75e97b4d309c70543d17bcb458": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f5cb57dddd6f4699a0214ff9cf59db9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9b02121b559e46a9a7aa32a4e18e67f9",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9551051,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9551051,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5329f0f590174acba197230bfefeaf07"
          }
        },
        "40a6209290a94ea1a3f89955a83a0563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_919e49eb87d54baeb10eae5944affcfa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 42.1M/? [00:01&lt;00:00, 28.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_64c092fc7d6b44c68e67417f9b049e03"
          }
        },
        "9b02121b559e46a9a7aa32a4e18e67f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5329f0f590174acba197230bfefeaf07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "919e49eb87d54baeb10eae5944affcfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "64c092fc7d6b44c68e67417f9b049e03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4539e55caccb4525b19af21ba4a17fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ac75f7827b8243c3bd80b9152e954bea",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_798f06f5e0ce4a69bf1cb1aa4dbe9e3e",
              "IPY_MODEL_f98e4c3ef3ff4c4babc5ca4e57440cd4"
            ]
          }
        },
        "ac75f7827b8243c3bd80b9152e954bea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "798f06f5e0ce4a69bf1cb1aa4dbe9e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_45fb050b16c049939e1184f897124770",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 800683,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 800683,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aa21189617f9436eaacfd59880878f7e"
          }
        },
        "f98e4c3ef3ff4c4babc5ca4e57440cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3596bf987d294961b8c01db0c9d18377",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.37M/? [00:00&lt;00:00, 9.33MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a77c93e76b9c4feeaee21de695acef7e"
          }
        },
        "45fb050b16c049939e1184f897124770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aa21189617f9436eaacfd59880878f7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3596bf987d294961b8c01db0c9d18377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a77c93e76b9c4feeaee21de695acef7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e3a985c077c43c485f9414ef9f39d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e3b55696f705446aab45e0e40234dd15",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b42269f019da4514897d97f9ffb6caca",
              "IPY_MODEL_59ee208e1377457cafa3125d27eafed0"
            ]
          }
        },
        "e3b55696f705446aab45e0e40234dd15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b42269f019da4514897d97f9ffb6caca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b1ec51c4dfd647b4812f48a4df51fc23",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c5212555459341a48acbf86e92918b04"
          }
        },
        "59ee208e1377457cafa3125d27eafed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f80102a63e8c422d8cecc663b0988c26",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 130319/0 [00:11&lt;00:00, 13047.75 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_60e842b458f64379b08d691005ba49b1"
          }
        },
        "b1ec51c4dfd647b4812f48a4df51fc23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5212555459341a48acbf86e92918b04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f80102a63e8c422d8cecc663b0988c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "60e842b458f64379b08d691005ba49b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb20612d4f2c40babad6cd6e88afc2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c1c41a74c7bf4a809f54507f94a7377e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5f0508026ee44c71a1423f6c6c3abffb",
              "IPY_MODEL_cfe42c1d9fae47feb8bea3ff1728bf6e"
            ]
          }
        },
        "c1c41a74c7bf4a809f54507f94a7377e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f0508026ee44c71a1423f6c6c3abffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_effd13c084bf43238fc47ca5671561db",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f7976589fa84bec9c7f51b550ab3723"
          }
        },
        "cfe42c1d9fae47feb8bea3ff1728bf6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dd9763ba2fc144c8b0c54fad17a3a784",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 11873/0 [00:01&lt;00:00, 151.63 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a8cc2b1bc12d4250ab06f6c914ef552f"
          }
        },
        "effd13c084bf43238fc47ca5671561db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f7976589fa84bec9c7f51b550ab3723": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd9763ba2fc144c8b0c54fad17a3a784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a8cc2b1bc12d4250ab06f6c914ef552f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "5cd1f5c864dc2481409d3eb3b782ed3b3fc4a29df12ee0f20630614ab7ab61ea"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Proyecto - SQuAD"
      ],
      "metadata": {
        "id": "457VPa20fZzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Benjamín Farías\n",
        "* Juan Hernández\n",
        "* Benjamín Lepe"
      ],
      "metadata": {
        "id": "IEpArQpx92oS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import torch\r\n",
        "import numpy as np\r\n",
        "import collections\r\n",
        "from random import random\r\n",
        "from tqdm import tqdm\r\n",
        "from pprint import pprint\r\n",
        "from torch import nn\r\n",
        "from torch.utils.data import random_split, Dataset, DataLoader\r\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW\r\n",
        "from datasets import load_dataset\r\n",
        "\r\n",
        "# Use GPU\r\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "# Reproducibility\r\n",
        "SEED = 1999\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "outputs": [],
      "metadata": {
        "id": "7JKafC0v92oV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# Load SQuAD 2.0 dataset\r\n",
        "squad_dataset = load_dataset('squad_v2')\r\n",
        "\r\n",
        "# Split into training/validation (from the training set)\r\n",
        "train_set, val_set = random_split(squad_dataset['train'], [117287, 13032])\r\n",
        "print(f'Training Set: {len(train_set)} examples')\r\n",
        "print(f'Validation Set: {len(val_set)} examples')\r\n",
        "\r\n",
        "# Testing set (in this case we use the dev set)\r\n",
        "test_set = squad_dataset['validation']\r\n",
        "print(f'Testing Set: {len(test_set)} examples')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset squad_v2 (C:\\Users\\benja\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0\\de2e67b822b2ef3f4b137148d0758f48075e3892c359c50271ef6c9add7e794a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set: 117287 examples\n",
            "Validation Set: 13032 examples\n",
            "Testing Set: 11873 examples\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "182a1546f291458ebd9af05460879505",
            "fcc84ae23ac84584a99ca37c69fa1e60",
            "4866f7da882045fab3693d29c7a91f1c",
            "e207cd6e8f4642588cdfe2b1c0a65408",
            "0189793570c04068af4f25846fc0b54f",
            "e0a23ba379ae47c385d80f7b7a83ff98",
            "7cf50f06c9df4faba683e88754fc1957",
            "65ccddd696db4003aa6b2adcda8d168c",
            "1d8e0b3e4318469d9641c2bd0980109a",
            "d47075d6fd8248b79e766dde90151ebe",
            "bf042832eed5449eba01b90db0e27972",
            "c579bc187fcd4d3a922d75e936e7a089",
            "e70b8e179ad148e59e73a435d57b2863",
            "f2a0a72a5ec54b979849e9a87be8c5d5",
            "a034558e746b4abe89c6471cb4704e8a",
            "8480a978d1ad498f88e33ea40bb720f0",
            "3233eb5925ef41d1838670755ef7010f",
            "2f9cfa75e97b4d309c70543d17bcb458",
            "f5cb57dddd6f4699a0214ff9cf59db9f",
            "40a6209290a94ea1a3f89955a83a0563",
            "9b02121b559e46a9a7aa32a4e18e67f9",
            "5329f0f590174acba197230bfefeaf07",
            "919e49eb87d54baeb10eae5944affcfa",
            "64c092fc7d6b44c68e67417f9b049e03",
            "4539e55caccb4525b19af21ba4a17fab",
            "ac75f7827b8243c3bd80b9152e954bea",
            "798f06f5e0ce4a69bf1cb1aa4dbe9e3e",
            "f98e4c3ef3ff4c4babc5ca4e57440cd4",
            "45fb050b16c049939e1184f897124770",
            "aa21189617f9436eaacfd59880878f7e",
            "3596bf987d294961b8c01db0c9d18377",
            "a77c93e76b9c4feeaee21de695acef7e",
            "4e3a985c077c43c485f9414ef9f39d4e",
            "e3b55696f705446aab45e0e40234dd15",
            "b42269f019da4514897d97f9ffb6caca",
            "59ee208e1377457cafa3125d27eafed0",
            "b1ec51c4dfd647b4812f48a4df51fc23",
            "c5212555459341a48acbf86e92918b04",
            "f80102a63e8c422d8cecc663b0988c26",
            "60e842b458f64379b08d691005ba49b1",
            "eb20612d4f2c40babad6cd6e88afc2b9",
            "c1c41a74c7bf4a809f54507f94a7377e",
            "5f0508026ee44c71a1423f6c6c3abffb",
            "cfe42c1d9fae47feb8bea3ff1728bf6e",
            "effd13c084bf43238fc47ca5671561db",
            "8f7976589fa84bec9c7f51b550ab3723",
            "dd9763ba2fc144c8b0c54fad17a3a784",
            "a8cc2b1bc12d4250ab06f6c914ef552f"
          ]
        },
        "id": "Ok4XnzlR92oa",
        "outputId": "a47dcf6b-f02c-418e-de27-a25a2d109161"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# Display information for specific example\r\n",
        "def display_example(example):\r\n",
        "    q = example['question']\r\n",
        "    c = example['context']\r\n",
        "    a = example['answers']['text']\r\n",
        "    print(f'Q: {q}\\n')\r\n",
        "    print('Context:')\r\n",
        "    pprint(c)\r\n",
        "    print(f'\\nTrue Answers:\\n{a}')"
      ],
      "outputs": [],
      "metadata": {
        "id": "hEMnF_l992oe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# Show example from evaluation set\r\n",
        "display_example(test_set[0])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: In what country is Normandy located?\n",
            "\n",
            "Context:\n",
            "('The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the '\n",
            " 'people who in the 10th and 11th centuries gave their name to Normandy, a '\n",
            " 'region in France. They were descended from Norse (\"Norman\" comes from '\n",
            " '\"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under '\n",
            " 'their leader Rollo, agreed to swear fealty to King Charles III of West '\n",
            " 'Francia. Through generations of assimilation and mixing with the native '\n",
            " 'Frankish and Roman-Gaulish populations, their descendants would gradually '\n",
            " 'merge with the Carolingian-based cultures of West Francia. The distinct '\n",
            " 'cultural and ethnic identity of the Normans emerged initially in the first '\n",
            " 'half of the 10th century, and it continued to evolve over the succeeding '\n",
            " 'centuries.')\n",
            "\n",
            "True Answers:\n",
            "['France', 'France', 'France', 'France']\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hisCfDU_92oh",
        "outputId": "920cc065-8d6b-4b66-84da-0ff3bb642bec"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# Extract info from dataset\r\n",
        "def get_info(dataset):\r\n",
        "    contexts = []\r\n",
        "    questions = []\r\n",
        "    answers = []\r\n",
        "    for example in dataset:\r\n",
        "        question = example['question']\r\n",
        "        context = example['context']\r\n",
        "        answer = {'text': '', 'answer_start': 0}\r\n",
        "        if not example['answers']['text']:\r\n",
        "            contexts.append(context)\r\n",
        "            questions.append(question)\r\n",
        "            answers.append({'text': '', 'answer_start': 0})\r\n",
        "        for ans_idx in range(len(example['answers']['text'])):\r\n",
        "            contexts.append(context)\r\n",
        "            questions.append(question)\r\n",
        "            answer = {'text': example['answers']['text'][ans_idx], 'answer_start': example['answers']['answer_start'][ans_idx]}\r\n",
        "            answers.append(answer)\r\n",
        "    return contexts, questions, answers\r\n",
        "\r\n",
        "train_contexts, train_questions, train_answers = get_info(train_set)\r\n",
        "val_contexts, val_questions, val_answers = get_info(val_set)"
      ],
      "outputs": [],
      "metadata": {
        "id": "G1Gz-IHT92ok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# Add index where each answer ends\r\n",
        "def add_end_idx(answers, contexts):\r\n",
        "    for answer, context in zip(answers, contexts):\r\n",
        "        gold_text = answer['text']\r\n",
        "        start_idx = answer['answer_start']\r\n",
        "        end_idx = start_idx + len(gold_text)\r\n",
        "\r\n",
        "        # Sometimes squad answers are off by a character or two – fix this\r\n",
        "        if context[start_idx : end_idx] == gold_text:\r\n",
        "            answer['answer_end'] = end_idx\r\n",
        "        elif context[start_idx - 1 : end_idx - 1] == gold_text:\r\n",
        "            answer['answer_start'] = start_idx - 1\r\n",
        "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\r\n",
        "        elif context[start_idx - 2 : end_idx - 2] == gold_text:\r\n",
        "            answer['answer_start'] = start_idx - 2\r\n",
        "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\r\n",
        "\r\n",
        "add_end_idx(train_answers, train_contexts)\r\n",
        "add_end_idx(val_answers, val_contexts)"
      ],
      "outputs": [],
      "metadata": {
        "id": "CnW6tvuQ92on"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# Add token positions to encodings\r\n",
        "def add_token_positions(encodings, answers, tokenizer):\r\n",
        "    start_positions = []\r\n",
        "    end_positions = []\r\n",
        "    for i in range(len(answers)):\r\n",
        "        if answers[i]['answer_end']:\r\n",
        "            start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\r\n",
        "            end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\r\n",
        "        else:\r\n",
        "            start_positions.append(0)\r\n",
        "            end_positions.append(0)\r\n",
        "\r\n",
        "        # If start position is None, the answer passage has been truncated\r\n",
        "        if start_positions[-1] is None:\r\n",
        "            start_positions[-1] = tokenizer.model_max_length\r\n",
        "        if end_positions[-1] is None:\r\n",
        "            end_positions[-1] = tokenizer.model_max_length\r\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\r\n",
        "\r\n",
        "# Tokenize the data\r\n",
        "def tokenize_data(contexts, questions, answers, tokenizer):\r\n",
        "    encodings = tokenizer(contexts, questions, truncation=True, padding='max_length', max_length=384)\r\n",
        "    add_token_positions(encodings, answers, tokenizer)\r\n",
        "    return encodings"
      ],
      "outputs": [],
      "metadata": {
        "id": "RQ3_bNs592oq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# SQuAD dataset features\r\n",
        "class SquadDataset(Dataset):\r\n",
        "    def __init__(self, contexts, questions, answers, tokenizer):\r\n",
        "        self.contexts = contexts\r\n",
        "        self.questions = questions\r\n",
        "        self.answers = answers\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        encoding = tokenize_data([self.contexts[idx]], [self.questions[idx]], [self.answers[idx]], self.tokenizer)\r\n",
        "        return {key: torch.tensor(val[0]) for key, val in encoding.items()}\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.contexts)\r\n",
        "\r\n",
        "# Tokenize datasets\r\n",
        "def prepare_features(tokenizer):\r\n",
        "    train_dataset = SquadDataset(train_contexts, train_questions, train_answers, tokenizer)\r\n",
        "    val_dataset = SquadDataset(val_contexts, val_questions, val_answers, tokenizer)\r\n",
        "    return train_dataset, val_dataset"
      ],
      "outputs": [],
      "metadata": {
        "id": "GalkGBQO92ov"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# Dict collate\r\n",
        "def dict_collate(batch):\r\n",
        "    group_dict = {key: [] for key in batch[0].keys()}\r\n",
        "    for item in batch:\r\n",
        "        for key, val in item.items():\r\n",
        "            if not val.dim():\r\n",
        "                group_dict[key].append(val.unsqueeze(0))\r\n",
        "            else:\r\n",
        "                group_dict[key].append(val)\r\n",
        "    return {key: torch.stack(val) for key, val in group_dict.items()}\r\n",
        "\r\n",
        "# Run training\r\n",
        "def run_training(model, train_set, val_set, args):\r\n",
        "    train_loader = DataLoader(train_set, batch_size=args['batch_size'], shuffle=True, collate_fn=dict_collate)\r\n",
        "    val_loader = DataLoader(val_set, batch_size=args['batch_size'], shuffle=True, collate_fn=dict_collate)\r\n",
        "    optim = AdamW(model.parameters(), lr=args['lr'])\r\n",
        "    history = {\r\n",
        "        'training': {'loss': []},\r\n",
        "        'validation': {'loss': []}\r\n",
        "    }\r\n",
        "    # Train for n_epochs\r\n",
        "    best_loss = float('inf')\r\n",
        "    for epoch in range(1, args['n_epochs'] + 1):\r\n",
        "        train_epoch_loss = run_epoch('train', model, train_loader, optimizer=optim, epoch=epoch, total_epoch=args['n_epochs'])\r\n",
        "        val_epoch_loss = run_epoch('val', model, val_loader, optimizer=optim, epoch=epoch, total_epoch=args['n_epochs'])\r\n",
        "\r\n",
        "        # Save loss/accuracy values for each epoch\r\n",
        "        history['training']['loss'].append(train_epoch_loss)\r\n",
        "        history['validation']['loss'].append(val_epoch_loss)\r\n",
        "\r\n",
        "        # Save model state if needed\r\n",
        "        if val_epoch_loss < best_loss:\r\n",
        "            best_loss = val_epoch_loss\r\n",
        "            torch.save(model.state_dict(), 'squad.pt')\r\n",
        "    return history\r\n",
        "\r\n",
        "# Run a single epoch\r\n",
        "def run_epoch(phase, model, loader, optimizer=None, epoch=0, total_epoch=0):\r\n",
        "    if phase == 'train':\r\n",
        "        model.train()\r\n",
        "    elif phase == 'val':\r\n",
        "        model.eval()\r\n",
        "    agg_loss = 0.0\r\n",
        "    with tqdm(loader, unit='batch', position=0, leave=True) as tepoch:\r\n",
        "        for n_batch, batch in enumerate(tepoch, start=1):\r\n",
        "            if phase == 'train': # Clean gradients on training\r\n",
        "                optimizer.zero_grad()\r\n",
        "                tepoch.set_description(f'Epoch {epoch}/{total_epoch}')\r\n",
        "            elif phase == 'val':\r\n",
        "                tepoch.set_description('Validating')\r\n",
        "\r\n",
        "            # Forward pass\r\n",
        "            input_ids = batch['input_ids'].to(device)\r\n",
        "            attention_mask = batch['attention_mask'].to(device)\r\n",
        "            start_positions = batch['start_positions'].to(device)\r\n",
        "            end_positions = batch['end_positions'].to(device)\r\n",
        "            if phase == 'val':\r\n",
        "                with torch.no_grad():\r\n",
        "                    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\r\n",
        "            else:\r\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\r\n",
        "\r\n",
        "            print(outputs)\r\n",
        "            loss = outputs[0]\r\n",
        "            agg_loss += loss.item()\r\n",
        "\r\n",
        "            # Update params\r\n",
        "            if phase == 'train':\r\n",
        "                loss.backward() # Backpropagation only while training\r\n",
        "                optimizer.step() # Update weights only while training\r\n",
        "            current_agg_loss = agg_loss / n_batch\r\n",
        "            tepoch.set_postfix(Loss=current_agg_loss)\r\n",
        "    epoch_loss = float(agg_loss / n_batch)\r\n",
        "    return epoch_loss"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZkQ6E4XQ92oz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# Load pre-trained model\r\n",
        "def get_model(name):\r\n",
        "    name_map = {\r\n",
        "        'bert': ['deepset/bert-base-cased-squad2', 'deepset/bert-base-cased-squad2'],\r\n",
        "        'roberta': ['deepset/roberta-base-squad2', 'deepset/roberta-base-squad2'],\r\n",
        "        'albert': ['twmkn9/albert-base-v2-squad2', 'albert-base-v2'],\r\n",
        "    }\r\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(name_map[name][0])\r\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name_map[name][1])\r\n",
        "    model.to(device)\r\n",
        "    return model, tokenizer"
      ],
      "outputs": [],
      "metadata": {
        "id": "bN2zbnr_92o1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "# ----------------- Metric functions for evaluation ----------------- #\r\n",
        "\r\n",
        "# Normalize text\r\n",
        "def normalize_text(s):\r\n",
        "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\r\n",
        "    import string, re\r\n",
        "\r\n",
        "    def remove_articles(text):\r\n",
        "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\r\n",
        "        return re.sub(regex, ' ', text)\r\n",
        "\r\n",
        "    def white_space_fix(text):\r\n",
        "        return ' '.join(text.split())\r\n",
        "\r\n",
        "    def remove_punc(text):\r\n",
        "        exclude = set(string.punctuation)\r\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\r\n",
        "\r\n",
        "    def lower(text):\r\n",
        "        return text.lower()\r\n",
        "\r\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\r\n",
        "\r\n",
        "# Exact match evaluation metric\r\n",
        "def compute_exact_match(prediction, truth):\r\n",
        "    return int(normalize_text(prediction) == normalize_text(truth))\r\n",
        "\r\n",
        "# F1 score evaluation metric\r\n",
        "def compute_f1(prediction, truth):\r\n",
        "    pred_tokens = normalize_text(prediction).split()\r\n",
        "    truth_tokens = normalize_text(truth).split()\r\n",
        "\r\n",
        "    # If either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\r\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\r\n",
        "        return int(pred_tokens == truth_tokens)\r\n",
        "\r\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\r\n",
        "\r\n",
        "    # If there are no common tokens then f1 = 0\r\n",
        "    if len(common_tokens) == 0:\r\n",
        "        return 0\r\n",
        "\r\n",
        "    prec = len(common_tokens) / len(pred_tokens)\r\n",
        "    rec = len(common_tokens) / len(truth_tokens)\r\n",
        "\r\n",
        "    return 2 * (prec * rec) / (prec + rec)\r\n",
        "\r\n",
        "# Retrieve possible answers\r\n",
        "def get_gold_answers(example):\r\n",
        "    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\r\n",
        "\r\n",
        "    gold_answers = [answer for answer in example['answers']['text'] if example['answers']['text']]\r\n",
        "\r\n",
        "    # If gold_answers doesn't exist it's because this is a negative example -\r\n",
        "    # the only correct answer is an empty string\r\n",
        "    if not gold_answers:\r\n",
        "        gold_answers = ['']\r\n",
        "\r\n",
        "    return gold_answers"
      ],
      "outputs": [],
      "metadata": {
        "id": "pNGSGHKk92o2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "source": [
        "# Obtain prediction for a specific question & context\r\n",
        "def get_prediction(model, example, tokenizer, nbest=10, null_threshold=1.0):\r\n",
        "    inputs = get_qa_inputs(example, tokenizer).to(device)\r\n",
        "    tokens = to_list(inputs['input_ids'])[0]\r\n",
        "    with torch.no_grad():\r\n",
        "        start_logits, end_logits = model(**inputs).values()  # Forward pass\r\n",
        "\r\n",
        "    # Get sensible preliminary predictions, sorted by score\r\n",
        "    prelim_preds = preliminary_predictions(start_logits, end_logits, inputs['input_ids'], nbest, tokenizer.sep_token_id)\r\n",
        "\r\n",
        "    # Narrow that down to the top nbest predictions\r\n",
        "    nbest_preds = best_predictions(prelim_preds, nbest, tokenizer, tokens, to_list(start_logits)[0], to_list(end_logits)[0])\r\n",
        "\r\n",
        "    # Compute the probability of each prediction\r\n",
        "    probabilities = prediction_probabilities(nbest_preds)\r\n",
        "\r\n",
        "    # Compute score difference\r\n",
        "    score_difference = compute_score_difference(nbest_preds)\r\n",
        "\r\n",
        "    # If score difference > threshold, return the null answer (for questions with no answer)\r\n",
        "    if score_difference > null_threshold:\r\n",
        "        return '', probabilities[-1]\r\n",
        "    else:\r\n",
        "        return nbest_preds[0].text, probabilities[0]\r\n",
        "\r\n",
        "# ----------------- Helper functions for get_prediction ----------------- #\r\n",
        "\r\n",
        "# Load the example, convert to inputs, get tokenized info\r\n",
        "def get_qa_inputs(example, tokenizer):\r\n",
        "    question = example['question']\r\n",
        "    context = example['context']\r\n",
        "    return tokenizer.encode_plus(question, context, return_tensors='pt', truncation=True, max_length=384)\r\n",
        "\r\n",
        "# Clean raw text\r\n",
        "def get_clean_text(tokens, tokenizer):\r\n",
        "    text = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(tokens))\r\n",
        "    text = text.strip()\r\n",
        "    text = ' '.join(text.split())\r\n",
        "    return text\r\n",
        "\r\n",
        "# Calculate probabilities for each prediction\r\n",
        "def prediction_probabilities(predictions):\r\n",
        "    def softmax(x):\r\n",
        "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\r\n",
        "        e_x = np.exp(x - np.max(x))\r\n",
        "        return e_x / e_x.sum()\r\n",
        "\r\n",
        "    all_scores = [pred.start_logit + pred.end_logit for pred in predictions]\r\n",
        "    return softmax(np.array(all_scores))\r\n",
        "\r\n",
        "# Convert tensor to list\r\n",
        "def to_list(tensor):\r\n",
        "    return tensor.detach().cpu().tolist()\r\n",
        "\r\n",
        "# Get preliminary predictions\r\n",
        "def preliminary_predictions(start_logits, end_logits, input_ids, nbest, sep_token_id):\r\n",
        "    # Convert tensors to lists\r\n",
        "    start_logits = to_list(start_logits)[0]\r\n",
        "    end_logits = to_list(end_logits)[0]\r\n",
        "    tokens = to_list(input_ids)[0]\r\n",
        "\r\n",
        "    # Sort our start and end logits from largest to smallest, keeping track of the index\r\n",
        "    start_idx_and_logit = sorted(enumerate(start_logits), key=lambda x: x[1], reverse=True)\r\n",
        "    end_idx_and_logit = sorted(enumerate(end_logits), key=lambda x: x[1], reverse=True)\r\n",
        "    start_indexes = [idx for idx, logit in start_idx_and_logit[:nbest]]\r\n",
        "    end_indexes = [idx for idx, logit in end_idx_and_logit[:nbest]]\r\n",
        "\r\n",
        "    # Question tokens are between the CLS token (101, at position 0) and first SEP (102) token\r\n",
        "    question_indexes = [i + 1 for i, token in enumerate(tokens[1 : tokens.index(sep_token_id)])]\r\n",
        "\r\n",
        "    # Keep track of all preliminary predictions\r\n",
        "    PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\r\n",
        "        'PrelimPrediction', ['start_index', 'end_index', 'start_logit', 'end_logit']\r\n",
        "    )\r\n",
        "    prelim_preds = []\r\n",
        "    for start_index in start_indexes:\r\n",
        "        for end_index in end_indexes:\r\n",
        "            # Throw out invalid predictions\r\n",
        "            if start_index in question_indexes:\r\n",
        "                continue\r\n",
        "            if end_index in question_indexes:\r\n",
        "                continue\r\n",
        "            if end_index < start_index:\r\n",
        "                continue\r\n",
        "            prelim_preds.append(\r\n",
        "                PrelimPrediction(\r\n",
        "                    start_index=start_index,\r\n",
        "                    end_index=end_index,\r\n",
        "                    start_logit=start_logits[start_index],\r\n",
        "                    end_logit=end_logits[end_index]\r\n",
        "                )\r\n",
        "            )\r\n",
        "    # Sort prelim_preds in descending score order\r\n",
        "    prelim_preds = sorted(prelim_preds, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\r\n",
        "    return prelim_preds\r\n",
        "\r\n",
        "# Filter the nbest predictions\r\n",
        "def best_predictions(prelim_preds, nbest, tokenizer, tokens, start_logits, end_logits):\r\n",
        "    # This will be the pool from which answer probabilities are computed\r\n",
        "    BestPrediction = collections.namedtuple(\r\n",
        "        'BestPrediction', ['text', 'start_logit', 'end_logit']\r\n",
        "    )\r\n",
        "    nbest_predictions = []\r\n",
        "    seen_predictions = []\r\n",
        "    for pred in prelim_preds:\r\n",
        "        if len(nbest_predictions) >= nbest:\r\n",
        "            break\r\n",
        "        if pred.start_index > 0: # Non-null answers\r\n",
        "            toks = tokens[pred.start_index : pred.end_index + 1]\r\n",
        "            text = get_clean_text(toks, tokenizer)\r\n",
        "\r\n",
        "            # If this text has been seen already - skip it\r\n",
        "            if text in seen_predictions:\r\n",
        "                continue\r\n",
        "\r\n",
        "            # Flag text as being seen\r\n",
        "            seen_predictions.append(text)\r\n",
        "\r\n",
        "            # Add this text to a pruned list of the top nbest predictions\r\n",
        "            nbest_predictions.append(\r\n",
        "                BestPrediction(\r\n",
        "                    text=text,\r\n",
        "                    start_logit=pred.start_logit,\r\n",
        "                    end_logit=pred.end_logit\r\n",
        "                )\r\n",
        "            )\r\n",
        "\r\n",
        "    # Add the null prediction\r\n",
        "    nbest_predictions.append(\r\n",
        "        BestPrediction(\r\n",
        "            text='',\r\n",
        "            start_logit=start_logits[0],\r\n",
        "            end_logit=end_logits[0]\r\n",
        "        )\r\n",
        "    )\r\n",
        "    return nbest_predictions\r\n",
        "\r\n",
        "# Calculate score to check if answer should be null\r\n",
        "def compute_score_difference(predictions):\r\n",
        "    \"\"\" Assumes that the null answer is always the last prediction \"\"\"\r\n",
        "    score_null = predictions[-1].start_logit + predictions[-1].end_logit\r\n",
        "    score_non_null = predictions[0].start_logit + predictions[0].end_logit\r\n",
        "    return score_null - score_non_null"
      ],
      "outputs": [],
      "metadata": {
        "id": "Qv2Ccr3F92o4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "# ----------------- Evaluation ----------------- #\r\n",
        "\r\n",
        "# Evaluate a single example\r\n",
        "def evaluate(model, example, tokenizer, nbest=10, null_threshold=-3.767639636993408):\r\n",
        "    model.eval()\r\n",
        "    prediction = get_prediction(model, example, tokenizer, nbest=nbest, null_threshold=null_threshold)\r\n",
        "    gold_answers = get_gold_answers(example)\r\n",
        "    em_score = max((compute_exact_match(prediction[0], answer)) for answer in gold_answers)\r\n",
        "    f1_score = max((compute_f1(prediction[0], answer)) for answer in gold_answers)\r\n",
        "    print(f'Context: {example[\"context\"]}\\n')\r\n",
        "    print(f'Question: {example[\"question\"]}')\r\n",
        "    print(f'Prediction: {prediction[0] if prediction[0] else \"NO ANSWER\"}')\r\n",
        "    print(f'True Answers: {gold_answers}')\r\n",
        "    print(f'EM: {em_score} \\t F1: {f1_score}')\r\n",
        "\r\n",
        "# Evaluate on the SQuAD dev set\r\n",
        "def run_testing(model, examples, tokenizer, nbest=10, null_threshold=-3.767639636993408):\r\n",
        "    model.eval()\r\n",
        "    em_score_total = 0\r\n",
        "    f1_score_total = 0\r\n",
        "    for example in examples:\r\n",
        "        prediction = get_prediction(model, example, tokenizer, nbest=nbest, null_threshold=null_threshold)\r\n",
        "        gold_answers = get_gold_answers(example)\r\n",
        "        em_score_total += max((compute_exact_match(prediction[0], answer)) for answer in gold_answers)\r\n",
        "        f1_score_total += max((compute_f1(prediction[0], answer)) for answer in gold_answers)\r\n",
        "    em_score_avg = round(100 * (em_score_total / len(examples)), 2)\r\n",
        "    f1_score_avg = round(100 * (f1_score_total / len(examples)), 2)\r\n",
        "    print(f'Avg EM: {em_score_avg}% \\t Avg F1: {f1_score_avg}%')"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ng7RqMwV92o9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "# RoBERTa model\r\n",
        "roberta_model, roberta_tokenizer = get_model('roberta')\r\n",
        "\r\n",
        "# ALBERT model\r\n",
        "albert_model, albert_tokenizer = get_model('albert')"
      ],
      "outputs": [],
      "metadata": {
        "id": "pSNgN-ar92o-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# Example evaluation\r\n",
        "example = test_set[0]\r\n",
        "evaluate(roberta_model, example, roberta_tokenizer)\r\n",
        "evaluate(albert_model, example, albert_tokenizer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\n",
            "\n",
            "Question: In what country is Normandy located?\n",
            "Prediction: France\n",
            "True Answers: ['France', 'France', 'France', 'France']\n",
            "EM: 1 \t F1: 1.0\n",
            "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\n",
            "\n",
            "Question: In what country is Normandy located?\n",
            "Prediction: france\n",
            "True Answers: ['France', 'France', 'France', 'France']\n",
            "EM: 1 \t F1: 1.0\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6vhwN2G92pA",
        "outputId": "6e89ff8b-9655-4cee-afe7-4be8a60a5ea5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "# Run testing with both models over SQuAD dev set\r\n",
        "#run_testing(roberta_model, test_set, roberta_tokenizer)\r\n",
        "#run_testing(albert_model, test_set, albert_tokenizer)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Q3Hv_bX692pC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "# Alberta ensemble model\r\n",
        "class AlbertaEnsemble(nn.Module):\r\n",
        "    def __init__(self, roberta_model, albert_model):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # RoBERTa model\r\n",
        "        self.roberta = roberta_model\r\n",
        "\r\n",
        "        # ALBERT model\r\n",
        "        self.albert = albert_model\r\n",
        "\r\n",
        "        # 1) En base a los input_ids y attention_mask elegir uno especializado\r\n",
        "        # 2) Pasar por los 2 modelos y decidir cuál usar con el cls (cls recibe outputs? o probabilidades?)\r\n",
        "\r\n",
        "    def forward(\r\n",
        "        self,\r\n",
        "        roberta_input_ids=None,\r\n",
        "        roberta_attention_mask=None,\r\n",
        "        roberta_start_positions=None,\r\n",
        "        roberta_end_positions=None,\r\n",
        "        albert_input_ids=None,\r\n",
        "        albert_attention_mask=None,\r\n",
        "        albert_token_type_ids=None,\r\n",
        "        albert_start_positions=None,\r\n",
        "        albert_end_positions=None,\r\n",
        "    ):\r\n",
        "        with torch.no_grad():\r\n",
        "            roberta_outputs = self.roberta(\r\n",
        "                input_ids=roberta_input_ids, attention_mask=roberta_attention_mask,\r\n",
        "                start_positions=roberta_start_positions, end_positions=roberta_end_positions\r\n",
        "            )\r\n",
        "\r\n",
        "            albert_outputs = self.albert(\r\n",
        "                input_ids=albert_input_ids, attention_mask=albert_attention_mask, token_type_ids=albert_token_type_ids,\r\n",
        "                start_positions=albert_start_positions, end_positions=albert_end_positions\r\n",
        "            )\r\n",
        "\r\n",
        "        chosen = random()\r\n",
        "        if chosen > 0.5:\r\n",
        "            return roberta_outputs, 'roberta'\r\n",
        "        return albert_outputs, 'albert'"
      ],
      "outputs": [],
      "metadata": {
        "id": "d5mIUQ1492pD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "# Run training for ensemble\r\n",
        "def run_training_ensemble(model, train_set_roberta, val_set_roberta, train_set_albert, val_set_albert, args):\r\n",
        "    # Dataloaders for Roberta\r\n",
        "    train_loader_roberta = DataLoader(train_set_roberta, batch_size=args['batch_size'], shuffle=False, collate_fn=dict_collate)\r\n",
        "    val_loader_roberta = DataLoader(val_set_roberta, batch_size=args['batch_size'], shuffle=False, collate_fn=dict_collate)\r\n",
        "\r\n",
        "    # Dataloaders for Albert\r\n",
        "    train_loader_albert = DataLoader(train_set_albert, batch_size=args['batch_size'], shuffle=False, collate_fn=dict_collate)\r\n",
        "    val_loader_albert = DataLoader(val_set_albert, batch_size=args['batch_size'], shuffle=False, collate_fn=dict_collate)\r\n",
        "\r\n",
        "    # Optimizer\r\n",
        "    optim = AdamW(model.parameters(), lr=args['lr'])\r\n",
        "\r\n",
        "    # History for epoch loss\r\n",
        "    history = {\r\n",
        "        'training': {'loss': []},\r\n",
        "        'validation': {'loss': []}\r\n",
        "    }\r\n",
        "\r\n",
        "    # Train for n_epochs\r\n",
        "    best_loss = float('inf')\r\n",
        "    for epoch in range(1, args['n_epochs'] + 1):\r\n",
        "        train_epoch_loss = run_epoch_ensemble('train', model, train_loader_roberta, train_loader_albert, optimizer=optim, epoch=epoch, total_epoch=args['n_epochs'])\r\n",
        "        val_epoch_loss = run_epoch_ensemble('val', model, val_loader_roberta, val_loader_albert, optimizer=optim, epoch=epoch, total_epoch=args['n_epochs'])\r\n",
        "\r\n",
        "        # Save loss/accuracy values for each epoch\r\n",
        "        history['training']['loss'].append(train_epoch_loss)\r\n",
        "        history['validation']['loss'].append(val_epoch_loss)\r\n",
        "\r\n",
        "        # Save model state if needed\r\n",
        "        if val_epoch_loss < best_loss:\r\n",
        "            best_loss = val_epoch_loss\r\n",
        "            torch.save(model.state_dict(), 'alberta.pt')\r\n",
        "    return history\r\n",
        "\r\n",
        "# Run a single epoch for ensemble\r\n",
        "def run_epoch_ensemble(phase, model, roberta_loader, albert_loader, optimizer=None, epoch=0, total_epoch=0):\r\n",
        "    if phase == 'train':\r\n",
        "        model.train()\r\n",
        "    elif phase == 'val':\r\n",
        "        model.eval()\r\n",
        "    agg_loss = 0.0\r\n",
        "    with tqdm(zip(roberta_loader, albert_loader), unit='batch', position=0, leave=True, total=len(roberta_loader)) as tepoch:\r\n",
        "        for n_batch, (roberta_batch, albert_batch) in enumerate(tepoch, start=1):\r\n",
        "            if phase == 'train': # Clean gradients on training\r\n",
        "                optimizer.zero_grad()\r\n",
        "                tepoch.set_description(f'Epoch {epoch}/{total_epoch}')\r\n",
        "            elif phase == 'val':\r\n",
        "                tepoch.set_description('Validating')\r\n",
        "\r\n",
        "            # Forward pass\r\n",
        "            roberta_input_ids = roberta_batch['input_ids'].to(device)\r\n",
        "            roberta_attention_mask = roberta_batch['attention_mask'].to(device)\r\n",
        "            roberta_start_pos = roberta_batch['start_positions'].to(device)\r\n",
        "            roberta_end_pos = roberta_batch['end_positions'].to(device)\r\n",
        "            albert_input_ids = albert_batch['input_ids'].to(device)\r\n",
        "            albert_attention_mask = albert_batch['attention_mask'].to(device)\r\n",
        "            albert_tokens = albert_batch['attention_mask'].to(device)\r\n",
        "            albert_start_pos = albert_batch['token_type_ids'].to(device)\r\n",
        "            albert_end_pos = albert_batch['end_positions'].to(device)\r\n",
        "            if phase == 'val':\r\n",
        "                with torch.no_grad():\r\n",
        "                    outputs = model(\r\n",
        "                        roberta_input_ids=roberta_input_ids, roberta_attention_mask=roberta_attention_mask,\r\n",
        "                        roberta_start_positions=roberta_start_pos, roberta_end_positions=roberta_end_pos,\r\n",
        "                        albert_input_ids=albert_input_ids, albert_attention_mask=albert_attention_mask,\r\n",
        "                        albert_token_type_ids=albert_tokens,\r\n",
        "                        albert_start_positions=albert_start_pos, albert_end_positions=albert_end_pos,\r\n",
        "                    )\r\n",
        "            else:\r\n",
        "                outputs = model(\r\n",
        "                    roberta_input_ids=roberta_input_ids, roberta_attention_mask=roberta_attention_mask,\r\n",
        "                    roberta_start_positions=roberta_start_pos, roberta_end_positions=roberta_end_pos,\r\n",
        "                    albert_input_ids=albert_input_ids, albert_attention_mask=albert_attention_mask,\r\n",
        "                    albert_token_type_ids=albert_tokens,\r\n",
        "                    albert_start_positions=albert_start_pos, albert_end_positions=albert_end_pos,\r\n",
        "                )\r\n",
        "\r\n",
        "            # Loss\r\n",
        "            loss = outputs[0]\r\n",
        "            agg_loss += loss.item()\r\n",
        "\r\n",
        "            # Update params\r\n",
        "            if phase == 'train':\r\n",
        "                loss.backward() # Backpropagation only while training\r\n",
        "                optimizer.step() # Update weights only while training\r\n",
        "            current_agg_loss = agg_loss / n_batch\r\n",
        "            tepoch.set_postfix(Loss=current_agg_loss)\r\n",
        "\r\n",
        "            # Save temporal checkpoints\r\n",
        "            if not (n_batch % 100):\r\n",
        "                torch.save(model.state_dict(), 'temp.pt')\r\n",
        "\r\n",
        "    epoch_loss = float(agg_loss / n_batch)\r\n",
        "    return epoch_loss"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "# ----------------- Ensemble Evaluation ----------------- #\r\n",
        "\r\n",
        "# Obtain prediction for a specific question & context (ensemble)\r\n",
        "def get_prediction_ensemble(model, example, roberta_tokenizer, albert_tokenizer, nbest=10, null_threshold=1.0):\r\n",
        "    roberta_inputs = get_qa_inputs(example, roberta_tokenizer).to(device)\r\n",
        "    albert_inputs = get_qa_inputs(example, albert_tokenizer).to(device)\r\n",
        "    input_map = {'roberta': roberta_inputs, 'albert': albert_inputs}\r\n",
        "    tokenizer_map = {'roberta': roberta_tokenizer, 'albert': albert_tokenizer}\r\n",
        "    with torch.no_grad():\r\n",
        "        output, chosen = model(\r\n",
        "                        roberta_input_ids=roberta_inputs['input_ids'], roberta_attention_mask=roberta_inputs['attention_mask'],\r\n",
        "                        albert_input_ids=albert_inputs['input_ids'], albert_attention_mask=albert_inputs['attention_mask'],\r\n",
        "                        albert_token_type_ids=albert_inputs['token_type_ids'],\r\n",
        "                    )\r\n",
        "    start_logits, end_logits = output.values()\r\n",
        "    tokens = to_list(input_map[chosen]['input_ids'])[0]\r\n",
        "    sep_token_id = tokenizer_map[chosen].sep_token_id\r\n",
        "\r\n",
        "    # Get sensible preliminary predictions, sorted by score\r\n",
        "    prelim_preds = preliminary_predictions(start_logits, end_logits, input_map[chosen]['input_ids'], nbest, sep_token_id)\r\n",
        "\r\n",
        "    # Narrow that down to the top nbest predictions\r\n",
        "    nbest_preds = best_predictions(prelim_preds, nbest, tokenizer_map[chosen], tokens, to_list(start_logits)[0], to_list(end_logits)[0])\r\n",
        "\r\n",
        "    # Compute the probability of each prediction\r\n",
        "    probabilities = prediction_probabilities(nbest_preds)\r\n",
        "\r\n",
        "    # Compute score difference\r\n",
        "    score_difference = compute_score_difference(nbest_preds)\r\n",
        "\r\n",
        "    # If score difference > threshold, return the null answer (for questions with no answer)\r\n",
        "    if score_difference > null_threshold:\r\n",
        "        return '', probabilities[-1]\r\n",
        "    else:\r\n",
        "        return nbest_preds[0].text, probabilities[0]\r\n",
        "\r\n",
        "# Evaluate a single example for ensemble\r\n",
        "def evaluate_ensemble(model, example, roberta_tokenizer, albert_tokenizer, nbest=10, null_threshold=-3.767639636993408):\r\n",
        "    model.eval()\r\n",
        "    prediction = get_prediction_ensemble(model, example, roberta_tokenizer, albert_tokenizer, nbest=nbest, null_threshold=null_threshold)\r\n",
        "    gold_answers = get_gold_answers(example)\r\n",
        "    em_score = max((compute_exact_match(prediction[0], answer)) for answer in gold_answers)\r\n",
        "    f1_score = max((compute_f1(prediction[0], answer)) for answer in gold_answers)\r\n",
        "    print(f'Context: {example[\"context\"]}\\n')\r\n",
        "    print(f'Question: {example[\"question\"]}')\r\n",
        "    print(f'Prediction: {prediction[0] if prediction[0] else \"NO ANSWER\"}')\r\n",
        "    print(f'True Answers: {gold_answers}')\r\n",
        "    print(f'EM: {em_score} \\t F1: {f1_score}')\r\n",
        "\r\n",
        "# Evaluate on the SQuAD dev set for ensemble\r\n",
        "def run_testing_ensemble(model, examples, roberta_tokenizer, albert_tokenizer, nbest=10, null_threshold=-3.767639636993408):\r\n",
        "    model.eval()\r\n",
        "    em_score_total = 0\r\n",
        "    f1_score_total = 0\r\n",
        "    for example in examples:\r\n",
        "        prediction = get_prediction_ensemble(model, example, roberta_tokenizer, albert_tokenizer, nbest=nbest, null_threshold=null_threshold)\r\n",
        "        gold_answers = get_gold_answers(example)\r\n",
        "        em_score_total += max((compute_exact_match(prediction[0], answer)) for answer in gold_answers)\r\n",
        "        f1_score_total += max((compute_f1(prediction[0], answer)) for answer in gold_answers)\r\n",
        "    em_score_avg = round(100 * (em_score_total / len(examples)), 2)\r\n",
        "    f1_score_avg = round(100 * (f1_score_total / len(examples)), 2)\r\n",
        "    print(f'Avg EM: {em_score_avg}% \\t Avg F1: {f1_score_avg}%')\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "alberta_ensemble = AlbertaEnsemble(roberta_model, albert_model).to(device)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "train_set_roberta, val_set_roberta = prepare_features(roberta_tokenizer)\r\n",
        "train_set_albert, val_set_albert = prepare_features(albert_tokenizer)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik4qaBeJ92pH",
        "outputId": "b3360129-44f4-425b-ed11-ba894421ec84"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "training_args = {\r\n",
        "    'batch_size': 2,\r\n",
        "    'lr': 3e-5,\r\n",
        "    'n_epochs': 1,\r\n",
        "}\r\n",
        "\r\n",
        "#loss_history = run_training_ensemble(alberta_ensemble, train_set_roberta, val_set_roberta, train_set_albert, val_set_albert, training_args)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "final_alberta = AlbertaEnsemble(roberta_model, albert_model).to(device)\r\n",
        "#final_alberta.load_state_dict(torch.load('temp.pt'))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "source": [
        "# Example evaluation\r\n",
        "example = test_set[0]\r\n",
        "evaluate_ensemble(final_alberta, example, roberta_tokenizer, albert_tokenizer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\n",
            "\n",
            "Question: In what country is Normandy located?\n",
            "Prediction: france\n",
            "True Answers: ['France', 'France', 'France', 'France']\n",
            "EM: 1 \t F1: 1.0\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "source": [
        "run_testing_ensemble(final_alberta, test_set, roberta_tokenizer, albert_tokenizer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg EM: 78.48% \t Avg F1: 81.33%\n"
          ]
        }
      ],
      "metadata": {}
    }
  ]
}