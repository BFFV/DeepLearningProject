{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"BERT.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.8.1 64-bit ('venv')"},"widgets":{"application/vnd.jupyter.widget-state+json":{"17108037891642fd8535f898ffc1c666":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_454c54b5f7e34caabfcdfb3751647f1d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2e1db44876aa4bda8f432424d43ef49d","IPY_MODEL_d2057bb0a03148d89ff752025b209034"]}},"454c54b5f7e34caabfcdfb3751647f1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2e1db44876aa4bda8f432424d43ef49d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_625f0bb8d61443ffa9aec2e2f52db015","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":443,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":443,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_658e95de041f4b6abbd70a8071253297"}},"d2057bb0a03148d89ff752025b209034":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_296fc3b5f35f49a4a8b71ba7450f39ba","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 443/443 [00:00&lt;00:00, 919B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0f4a53ffb84641be966022b5f995d597"}},"625f0bb8d61443ffa9aec2e2f52db015":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"658e95de041f4b6abbd70a8071253297":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"296fc3b5f35f49a4a8b71ba7450f39ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0f4a53ffb84641be966022b5f995d597":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0325dd797ca640cea44dc5d2d50d3378":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a5d823c6c36f4ffaa54de63b7df27187","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bdec4b4708b1446bbeb00ddef16511c7","IPY_MODEL_183df2bf188a4f9fba59234e57b205e4"]}},"a5d823c6c36f4ffaa54de63b7df27187":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bdec4b4708b1446bbeb00ddef16511c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_eb910005a7ae4e04ab28ac8d506e9f9c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1340675298,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1340675298,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b871a4177dba4977813c16a6a71d3b27"}},"183df2bf188a4f9fba59234e57b205e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3f1e67a35637482e9a2db7e7233d02c2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.34G/1.34G [00:22&lt;00:00, 58.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ba7cd4be6dcb457db7197cd326a56793"}},"eb910005a7ae4e04ab28ac8d506e9f9c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b871a4177dba4977813c16a6a71d3b27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3f1e67a35637482e9a2db7e7233d02c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ba7cd4be6dcb457db7197cd326a56793":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"604df6fab59c46438be9ac909632e6fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_54decd45844c47ea8c2816ef729e7445","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9c50285033b941c19bb0d6cfefa35a92","IPY_MODEL_9cdafc71790e4a65afdea6267db7ea29"]}},"54decd45844c47ea8c2816ef729e7445":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9c50285033b941c19bb0d6cfefa35a92":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7773da342fd34ddd8d98d22308ff84a0","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_15cf214d3f95421790d246a7fdc8d200"}},"9cdafc71790e4a65afdea6267db7ea29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2c9c2fd7eece41589d73d9a238225905","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 739kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c7052a47457d44e2b4914734294eb671"}},"7773da342fd34ddd8d98d22308ff84a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"15cf214d3f95421790d246a7fdc8d200":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2c9c2fd7eece41589d73d9a238225905":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c7052a47457d44e2b4914734294eb671":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"language_info":{"name":"python","version":"3.8.1","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"5cd1f5c864dc2481409d3eb3b782ed3b3fc4a29df12ee0f20630614ab7ab61ea"}},"cells":[{"cell_type":"markdown","source":["# Proyecto - SQuAD"],"metadata":{"id":"457VPa20fZzY"}},{"cell_type":"markdown","source":["* Benjamín Farías\r\n","* Juan Hernández\r\n","* Benjamín Lepe"],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["import torch\r\n","import numpy as np\r\n","import collections\r\n","from pprint import pprint\r\n","from torch.utils.data import random_split\r\n","from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\r\n","from datasets import load_dataset\r\n","\r\n","# Use GPU\r\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n","\r\n","# Reproducibility\r\n","SEED = 1\r\n","torch.manual_seed(SEED)\r\n","torch.backends.cudnn.deterministic = True"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["squad_dataset = load_dataset('squad_v2')"],"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: 5.26kB [00:00, 5.28MB/s]                   \n","Downloading: 2.40kB [00:00, 1.81MB/s]                   \n"]},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to C:\\Users\\benja\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0\\de2e67b822b2ef3f4b137148d0758f48075e3892c359c50271ef6c9add7e794a...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: 42.1MB [00:04, 9.81MB/s]\n","Downloading: 4.37MB [00:00, 69.4MB/s]                  \n","                                           "]},{"output_type":"stream","name":"stdout","text":["Dataset squad_v2 downloaded and prepared to C:\\Users\\benja\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0\\de2e67b822b2ef3f4b137148d0758f48075e3892c359c50271ef6c9add7e794a. Subsequent calls will reuse this data.\n"]},{"output_type":"stream","name":"stderr","text":[]}],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["# Split into training/validation\r\n","train_set, val_set = random_split(squad_dataset['train'], [117287, 13032])\r\n","test_set = squad_dataset['validation']"],"outputs":[{"output_type":"stream","name":"stdout","text":["117287\n","13032\n","11873\n"]}],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["# Generate some maps to help us identify examples of interest\r\n","qid_to_example_index = {example.qas_id: i for i, example in enumerate(test_examples)}\r\n","qid_to_has_answer = {example.qas_id: bool(example.answers) for example in test_examples}\r\n","answer_qids = [qas_id for qas_id, has_answer in qid_to_has_answer.items() if has_answer]\r\n","no_answer_qids = [qas_id for qas_id, has_answer in qid_to_has_answer.items() if not has_answer]"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["# Display information for specific example\r\n","def display_example(qid, examples):\r\n","    idx = qid_to_example_index[qid]\r\n","    q = examples[idx].question_text\r\n","    c = examples[idx].context_text\r\n","    a = [answer['text'] for answer in examples[idx].answers]\r\n","    print(f'Example {idx} of {len(examples)}\\n---------------------')\r\n","    print(f'Q: {q}\\n')\r\n","    print('Context:')\r\n","    pprint(c)\r\n","    print(f'\\nTrue Answers:\\n{a}')"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["# Show example with true answers\r\n","display_example(answer_qids[1300], test_examples)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Example 2548 of 11873\n","---------------------\n","Q: Where on Earth is free oxygen found?\n","\n","Context:\n","(\"Free oxygen also occurs in solution in the world's water bodies. The \"\n"," 'increased solubility of O\\n'\n"," '2 at lower temperatures (see Physical properties) has important implications '\n"," 'for ocean life, as polar oceans support a much higher density of life due to '\n"," 'their higher oxygen content. Water polluted with plant nutrients such as '\n"," 'nitrates or phosphates may stimulate growth of algae by a process called '\n"," 'eutrophication and the decay of these organisms and other biomaterials may '\n"," 'reduce amounts of O\\n'\n"," '2 in eutrophic water bodies. Scientists assess this aspect of water quality '\n"," \"by measuring the water's biochemical oxygen demand, or the amount of O\\n\"\n"," '2 needed to restore it to a normal concentration.')\n","\n","True Answers:\n","['water', \"in solution in the world's water bodies\", \"the world's water bodies\"]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["# Show example with no answer\r\n","display_example(no_answer_qids[1254], test_examples)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Example 2564 of 11873\n","---------------------\n","Q: What happened 3.7-2 billion years ago?\n","\n","Context:\n","(\"Free oxygen gas was almost nonexistent in Earth's atmosphere before \"\n"," 'photosynthetic archaea and bacteria evolved, probably about 3.5 billion '\n"," 'years ago. Free oxygen first appeared in significant quantities during the '\n"," 'Paleoproterozoic eon (between 3.0 and 2.3 billion years ago). For the first '\n"," 'billion years, any free oxygen produced by these organisms combined with '\n"," 'dissolved iron in the oceans to form banded iron formations. When such '\n"," 'oxygen sinks became saturated, free oxygen began to outgas from the oceans '\n"," '3–2.7 billion years ago, reaching 10% of its present level around 1.7 '\n"," 'billion years ago.')\n","\n","True Answers:\n","[]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Normalize text\r\n","def normalize_text(s):\r\n","    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\r\n","    import string, re\r\n","\r\n","    def remove_articles(text):\r\n","        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\r\n","        return re.sub(regex, ' ', text)\r\n","\r\n","    def white_space_fix(text):\r\n","        return ' '.join(text.split())\r\n","\r\n","    def remove_punc(text):\r\n","        exclude = set(string.punctuation)\r\n","        return ''.join(ch for ch in text if ch not in exclude)\r\n","\r\n","    def lower(text):\r\n","        return text.lower()\r\n","\r\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\r\n","\r\n","# Exact match evaluation metric\r\n","def compute_exact_match(prediction, truth):\r\n","    return int(normalize_text(prediction) == normalize_text(truth))\r\n","\r\n","# F1 score evaluation metric\r\n","def compute_f1(prediction, truth):\r\n","    pred_tokens = normalize_text(prediction).split()\r\n","    truth_tokens = normalize_text(truth).split()\r\n","\r\n","    # If either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\r\n","    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\r\n","        return int(pred_tokens == truth_tokens)\r\n","\r\n","    common_tokens = set(pred_tokens) & set(truth_tokens)\r\n","\r\n","    # If there are no common tokens then f1 = 0\r\n","    if len(common_tokens) == 0:\r\n","        return 0\r\n","\r\n","    prec = len(common_tokens) / len(pred_tokens)\r\n","    rec = len(common_tokens) / len(truth_tokens)\r\n","\r\n","    return 2 * (prec * rec) / (prec + rec)\r\n","\r\n","# Retrieve possible answers\r\n","def get_gold_answers(example):\r\n","    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\r\n","\r\n","    gold_answers = [answer['text'] for answer in example.answers if answer['text']]\r\n","\r\n","    # If gold_answers doesn't exist it's because this is a negative example -\r\n","    # the only correct answer is an empty string\r\n","    if not gold_answers:\r\n","        gold_answers = ['']\r\n","\r\n","    return gold_answers"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Obtain prediction for a specific question & context\r\n","def get_prediction(model, example, tokenizer, nbest=10, null_threshold=1.0):\r\n","    inputs = get_qa_inputs(example, tokenizer).to(device)\r\n","    tokens = to_list(inputs['input_ids'])[0]\r\n","    with torch.no_grad():\r\n","        start_logits, end_logits = model(**inputs).values()  # Forward pass\r\n","\r\n","    # Get sensible preliminary predictions, sorted by score\r\n","    prelim_preds = preliminary_predictions(start_logits, end_logits, inputs['input_ids'], nbest, tokenizer.sep_token_id)\r\n","\r\n","    # Narrow that down to the top nbest predictions\r\n","    nbest_preds = best_predictions(prelim_preds, nbest, tokenizer, tokens, to_list(start_logits)[0], to_list(end_logits)[0])\r\n","\r\n","    # Compute the probability of each prediction\r\n","    probabilities = prediction_probabilities(nbest_preds)\r\n","\r\n","    # Compute score difference\r\n","    score_difference = compute_score_difference(nbest_preds)\r\n","\r\n","    # If score difference > threshold, return the null answer (for questions with no answer)\r\n","    if score_difference > null_threshold:\r\n","        return '', probabilities[-1]\r\n","    else:\r\n","        return nbest_preds[0].text, probabilities[0]\r\n","\r\n","# ----------------- Helper functions for get_prediction ----------------- #\r\n","\r\n","# Load the example, convert to inputs, get tokenized info\r\n","def get_qa_inputs(example, tokenizer):\r\n","    question = example.question_text\r\n","    context = example.context_text\r\n","    return tokenizer.encode_plus(question, context, return_tensors='pt', truncation=True, max_length=512)\r\n","\r\n","# Clean raw text\r\n","def get_clean_text(tokens, tokenizer):\r\n","    text = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(tokens))\r\n","    text = text.strip()\r\n","    text = ' '.join(text.split())\r\n","    return text\r\n","\r\n","# Calculate probabilities for each prediction\r\n","def prediction_probabilities(predictions):\r\n","    def softmax(x):\r\n","        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\r\n","        e_x = np.exp(x - np.max(x))\r\n","        return e_x / e_x.sum()\r\n","\r\n","    all_scores = [pred.start_logit + pred.end_logit for pred in predictions]\r\n","    return softmax(np.array(all_scores))\r\n","\r\n","# Convert tensor to list\r\n","def to_list(tensor):\r\n","    return tensor.detach().cpu().tolist()\r\n","\r\n","# Get preliminary predictions\r\n","def preliminary_predictions(start_logits, end_logits, input_ids, nbest, sep_token_id):\r\n","    # Convert tensors to lists\r\n","    start_logits = to_list(start_logits)[0]\r\n","    end_logits = to_list(end_logits)[0]\r\n","    tokens = to_list(input_ids)[0]\r\n","\r\n","    # Sort our start and end logits from largest to smallest, keeping track of the index\r\n","    start_idx_and_logit = sorted(enumerate(start_logits), key=lambda x: x[1], reverse=True)\r\n","    end_idx_and_logit = sorted(enumerate(end_logits), key=lambda x: x[1], reverse=True)\r\n","    start_indexes = [idx for idx, logit in start_idx_and_logit[:nbest]]\r\n","    end_indexes = [idx for idx, logit in end_idx_and_logit[:nbest]]\r\n","\r\n","    # Question tokens are between the CLS token (101, at position 0) and first SEP (102) token\r\n","    question_indexes = [i + 1 for i, token in enumerate(tokens[1 : tokens.index(sep_token_id)])]\r\n","\r\n","    # Keep track of all preliminary predictions\r\n","    PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\r\n","        'PrelimPrediction', ['start_index', 'end_index', 'start_logit', 'end_logit']\r\n","    )\r\n","    prelim_preds = []\r\n","    for start_index in start_indexes:\r\n","        for end_index in end_indexes:\r\n","            # Throw out invalid predictions\r\n","            if start_index in question_indexes:\r\n","                continue\r\n","            if end_index in question_indexes:\r\n","                continue\r\n","            if end_index < start_index:\r\n","                continue\r\n","            prelim_preds.append(\r\n","                PrelimPrediction(\r\n","                    start_index=start_index,\r\n","                    end_index=end_index,\r\n","                    start_logit=start_logits[start_index],\r\n","                    end_logit=end_logits[end_index]\r\n","                )\r\n","            )\r\n","    # Sort prelim_preds in descending score order\r\n","    prelim_preds = sorted(prelim_preds, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\r\n","    return prelim_preds\r\n","\r\n","# Filter the nbest predictions\r\n","def best_predictions(prelim_preds, nbest, tokenizer, tokens, start_logits, end_logits):\r\n","    # This will be the pool from which answer probabilities are computed\r\n","    BestPrediction = collections.namedtuple(\r\n","        'BestPrediction', ['text', 'start_logit', 'end_logit']\r\n","    )\r\n","    nbest_predictions = []\r\n","    seen_predictions = []\r\n","    for pred in prelim_preds:\r\n","        if len(nbest_predictions) >= nbest:\r\n","            break\r\n","        if pred.start_index > 0: # Non-null answers\r\n","            toks = tokens[pred.start_index : pred.end_index + 1]\r\n","            text = get_clean_text(toks, tokenizer)\r\n","\r\n","            # If this text has been seen already - skip it\r\n","            if text in seen_predictions:\r\n","                continue\r\n","\r\n","            # Flag text as being seen\r\n","            seen_predictions.append(text)\r\n","\r\n","            # Add this text to a pruned list of the top nbest predictions\r\n","            nbest_predictions.append(\r\n","                BestPrediction(\r\n","                    text=text,\r\n","                    start_logit=pred.start_logit,\r\n","                    end_logit=pred.end_logit\r\n","                )\r\n","            )\r\n","\r\n","    # Add the null prediction\r\n","    nbest_predictions.append(\r\n","        BestPrediction(\r\n","            text='',\r\n","            start_logit=start_logits[0],\r\n","            end_logit=end_logits[0]\r\n","        )\r\n","    )\r\n","    return nbest_predictions\r\n","\r\n","# Calculate score to check if answer should be null\r\n","def compute_score_difference(predictions):\r\n","    \"\"\" Assumes that the null answer is always the last prediction \"\"\"\r\n","    score_null = predictions[-1].start_logit + predictions[-1].end_logit\r\n","    score_non_null = predictions[0].start_logit + predictions[0].end_logit\r\n","    return score_null - score_non_null"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Evaluate a single example\r\n","def evaluate(model, example, tokenizer, nbest=10, null_threshold=-3.767639636993408):\r\n","    model.to(device)\r\n","    model.eval()\r\n","    prediction = get_prediction(model, example, tokenizer, nbest=nbest, null_threshold=null_threshold)\r\n","    gold_answers = get_gold_answers(example)\r\n","    em_score = max((compute_exact_match(prediction[0], answer)) for answer in gold_answers)\r\n","    f1_score = max((compute_f1(prediction[0], answer)) for answer in gold_answers)\r\n","    print(f'Context: {example.context_text}\\n')\r\n","    print(f'Question: {example.question_text}')\r\n","    print(f'Prediction: {prediction[0] if prediction[0] else \"NO ANSWER\"}')\r\n","    print(f'True Answers: {gold_answers}')\r\n","    print(f'EM: {em_score} \\t F1: {f1_score}')\r\n","\r\n","# Evaluate on the SQuAD dev set\r\n","def run_testing(model, examples, tokenizer, nbest=10, null_threshold=-3.767639636993408):\r\n","    model.to(device)\r\n","    model.eval()\r\n","    em_score_total = 0\r\n","    f1_score_total = 0\r\n","    for example in examples:\r\n","        prediction = get_prediction(model, example, tokenizer, nbest=nbest, null_threshold=null_threshold)\r\n","        gold_answers = get_gold_answers(example)\r\n","        em_score_total += max((compute_exact_match(prediction[0], answer)) for answer in gold_answers)\r\n","        f1_score_total += max((compute_f1(prediction[0], answer)) for answer in gold_answers)\r\n","    em_score_avg = round(100 * (em_score_total / len(examples)), 2)\r\n","    f1_score_avg = round(100 * (f1_score_total / len(examples)), 2)\r\n","    print(f'Avg EM: {em_score_avg}% \\t Avg F1: {f1_score_avg}%')"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["# Load pre-trained model\r\n","def get_model(name):\r\n","    name_map = {\r\n","        'bert': 'deepset/bert-base-cased-squad2',\r\n","        'bert_wwm': 'deepset/bert-large-uncased-whole-word-masking-squad2',\r\n","        'roberta': 'deepset/roberta-base-squad2',\r\n","        'sapbert': 'bigwiz83/sapbert-from-pubmedbert-squad2'\r\n","    }\r\n","    tokenizer = AutoTokenizer.from_pretrained(name_map[name])\r\n","    model = AutoModelForQuestionAnswering.from_pretrained(name_map[name])\r\n","    return model, tokenizer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["# Tokenize sets for training & evaluation\r\n","def tokenize_set(tokenizer):\r\n","    return\r\n","\r\n","# Train on the SQuAD training set\r\n","def run_training(model, tokenizer, train_set, eval_set, args):\r\n","    model.to(device)\r\n","    trainer = Trainer(\r\n","        model,\r\n","        args,\r\n","        train_dataset=train_set,\r\n","        eval_dataset=eval_set,\r\n","        tokenizer=tokenizer\r\n","    )\r\n","    trainer.train()\r\n","    #trainer.save_model('squad')"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["training_args = TrainingArguments(\r\n","    'checkpoints',\r\n","    evaluation_strategy='epoch',\r\n","    learning_rate=3e-5,\r\n","    per_device_train_batch_size=8,\r\n","    per_device_eval_batch_size=8,\r\n","    num_train_epochs=2\r\n",")"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["# Models: bert, bert_wwm, roberta, sapbert\r\n","model, tokenizer = get_model('bert')"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":16,"source":["run_training(model, tokenizer, train_set, val_set, training_args)"],"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 117287\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 29322\n","  0%|          | 0/29322 [00:00<?, ?it/s]"]},{"output_type":"error","ename":"ValueError","evalue":"You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['id', 'title', 'context', 'question', 'answers']","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m<ipython-input-16-2af035d03870>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m<ipython-input-13-126576145e3f>\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m(model, tokenizer, train_set, eval_set, args)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     )\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m#trainer.save_model('squad')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32md:\\Benja\\Desktop\\Universidad\\2021\\1er Semestre\\Aprendizaje Profundo\\DeepLearningProject\\venv\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[0;32m   1241\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1243\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32md:\\Benja\\Desktop\\Universidad\\2021\\1er Semestre\\Aprendizaje Profundo\\DeepLearningProject\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32md:\\Benja\\Desktop\\Universidad\\2021\\1er Semestre\\Aprendizaje Profundo\\DeepLearningProject\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32md:\\Benja\\Desktop\\Universidad\\2021\\1er Semestre\\Aprendizaje Profundo\\DeepLearningProject\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32md:\\Benja\\Desktop\\Universidad\\2021\\1er Semestre\\Aprendizaje Profundo\\DeepLearningProject\\venv\\lib\\site-packages\\transformers\\data\\data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         batch = self.tokenizer.pad(\n\u001b[0m\u001b[0;32m    119\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32md:\\Benja\\Desktop\\Universidad\\2021\\1er Semestre\\Aprendizaje Profundo\\DeepLearningProject\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   2618\u001b[0m         \u001b[1;31m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2619\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_input_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mencoded_inputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2620\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   2621\u001b[0m                 \u001b[1;34m\"You should supply an encoding or a list of encodings to this method \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2622\u001b[0m                 \u001b[1;34mf\"that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['id', 'title', 'context', 'question', 'answers']"]}],"metadata":{}},{"cell_type":"code","execution_count":41,"source":["# Positive example evaluation\r\n","example = test_examples[qid_to_example_index[answer_qids[1300]]]\r\n","evaluate(model, example, tokenizer)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Context: Free oxygen also occurs in solution in the world's water bodies. The increased solubility of O\n","2 at lower temperatures (see Physical properties) has important implications for ocean life, as polar oceans support a much higher density of life due to their higher oxygen content. Water polluted with plant nutrients such as nitrates or phosphates may stimulate growth of algae by a process called eutrophication and the decay of these organisms and other biomaterials may reduce amounts of O\n","2 in eutrophic water bodies. Scientists assess this aspect of water quality by measuring the water's biochemical oxygen demand, or the amount of O\n","2 needed to restore it to a normal concentration.\n","\n","Question: Where on Earth is free oxygen found?\n","Prediction: water bodies\n","True Answers: ['water', \"in solution in the world's water bodies\", \"the world's water bodies\"]\n","EM: 0 \t F1: 0.8\n"]}],"metadata":{}},{"cell_type":"code","execution_count":42,"source":["# Negative example evaluation\r\n","example = test_examples[qid_to_example_index[no_answer_qids[1254]]]\r\n","evaluate(model, example, tokenizer)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Context: Free oxygen gas was almost nonexistent in Earth's atmosphere before photosynthetic archaea and bacteria evolved, probably about 3.5 billion years ago. Free oxygen first appeared in significant quantities during the Paleoproterozoic eon (between 3.0 and 2.3 billion years ago). For the first billion years, any free oxygen produced by these organisms combined with dissolved iron in the oceans to form banded iron formations. When such oxygen sinks became saturated, free oxygen began to outgas from the oceans 3–2.7 billion years ago, reaching 10% of its present level around 1.7 billion years ago.\n","\n","Question: What happened 3.7-2 billion years ago?\n","Prediction: NO ANSWER\n","True Answers: ['']\n","EM: 1 \t F1: 1\n"]}],"metadata":{}},{"cell_type":"code","execution_count":43,"source":["# Run testing over SQuAD dev set\r\n","run_testing(model, test_examples, tokenizer)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Avg EM: 74.34% \t Avg F1: 78.34%\n"]}],"metadata":{}}]}