{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"BERT.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.8.1 64-bit ('venv')"},"widgets":{"application/vnd.jupyter.widget-state+json":{"17108037891642fd8535f898ffc1c666":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_454c54b5f7e34caabfcdfb3751647f1d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2e1db44876aa4bda8f432424d43ef49d","IPY_MODEL_d2057bb0a03148d89ff752025b209034"]}},"454c54b5f7e34caabfcdfb3751647f1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2e1db44876aa4bda8f432424d43ef49d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_625f0bb8d61443ffa9aec2e2f52db015","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":443,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":443,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_658e95de041f4b6abbd70a8071253297"}},"d2057bb0a03148d89ff752025b209034":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_296fc3b5f35f49a4a8b71ba7450f39ba","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 443/443 [00:00&lt;00:00, 919B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0f4a53ffb84641be966022b5f995d597"}},"625f0bb8d61443ffa9aec2e2f52db015":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"658e95de041f4b6abbd70a8071253297":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"296fc3b5f35f49a4a8b71ba7450f39ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0f4a53ffb84641be966022b5f995d597":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0325dd797ca640cea44dc5d2d50d3378":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a5d823c6c36f4ffaa54de63b7df27187","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bdec4b4708b1446bbeb00ddef16511c7","IPY_MODEL_183df2bf188a4f9fba59234e57b205e4"]}},"a5d823c6c36f4ffaa54de63b7df27187":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bdec4b4708b1446bbeb00ddef16511c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_eb910005a7ae4e04ab28ac8d506e9f9c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1340675298,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1340675298,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b871a4177dba4977813c16a6a71d3b27"}},"183df2bf188a4f9fba59234e57b205e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3f1e67a35637482e9a2db7e7233d02c2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.34G/1.34G [00:22&lt;00:00, 58.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ba7cd4be6dcb457db7197cd326a56793"}},"eb910005a7ae4e04ab28ac8d506e9f9c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b871a4177dba4977813c16a6a71d3b27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3f1e67a35637482e9a2db7e7233d02c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ba7cd4be6dcb457db7197cd326a56793":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"604df6fab59c46438be9ac909632e6fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_54decd45844c47ea8c2816ef729e7445","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9c50285033b941c19bb0d6cfefa35a92","IPY_MODEL_9cdafc71790e4a65afdea6267db7ea29"]}},"54decd45844c47ea8c2816ef729e7445":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9c50285033b941c19bb0d6cfefa35a92":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7773da342fd34ddd8d98d22308ff84a0","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_15cf214d3f95421790d246a7fdc8d200"}},"9cdafc71790e4a65afdea6267db7ea29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2c9c2fd7eece41589d73d9a238225905","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 739kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c7052a47457d44e2b4914734294eb671"}},"7773da342fd34ddd8d98d22308ff84a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"15cf214d3f95421790d246a7fdc8d200":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2c9c2fd7eece41589d73d9a238225905":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c7052a47457d44e2b4914734294eb671":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"language_info":{"name":"python","version":"3.8.1","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"5cd1f5c864dc2481409d3eb3b782ed3b3fc4a29df12ee0f20630614ab7ab61ea"}},"cells":[{"cell_type":"markdown","source":["# Proyecto - SQuAD"],"metadata":{"id":"457VPa20fZzY"}},{"cell_type":"markdown","source":["* Benjamín Farías\r\n","* Juan Hernández\r\n","* Benjamín Lepe"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import torch\r\n","import numpy as np\r\n","import collections\r\n","from tqdm import tqdm\r\n","from pprint import pprint\r\n","from torch import nn\r\n","from torch.utils.data import random_split, Dataset, DataLoader\r\n","from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW, PreTrainedModel, BertModel\r\n","from datasets import load_dataset\r\n","\r\n","# Use GPU\r\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n","\r\n","# Reproducibility\r\n","SEED = 1999\r\n","torch.manual_seed(SEED)\r\n","torch.backends.cudnn.deterministic = True"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Load SQuAD 2.0 dataset\r\n","squad_dataset = load_dataset('squad_v2')\r\n","\r\n","# Split into training/validation (from the training set)\r\n","train_set, val_set = random_split(squad_dataset['train'], [117287, 13032])\r\n","print(f'Training Set: {len(train_set)} examples')\r\n","print(f'Validation Set: {len(val_set)} examples')\r\n","\r\n","# Testing set (in this case we use the dev set)\r\n","test_set = squad_dataset['validation']\r\n","print(f'Testing Set: {len(test_set)} examples')"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Display information for specific example\r\n","def display_example(example):\r\n","    q = example['question']\r\n","    c = example['context']\r\n","    a = example['answers']['text']\r\n","    print(f'Q: {q}\\n')\r\n","    print('Context:')\r\n","    pprint(c)\r\n","    print(f'\\nTrue Answers:\\n{a}')"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Show example from evaluation set\r\n","display_example(test_set[0])"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Extract info from dataset\r\n","def get_info(dataset):\r\n","    contexts = []\r\n","    questions = []\r\n","    answers = []\r\n","    for example in dataset:\r\n","        question = example['question']\r\n","        context = example['context']\r\n","        answer = {'text': '', 'answer_start': 0}\r\n","        if not example['answers']['text']:\r\n","            contexts.append(context)\r\n","            questions.append(question)\r\n","            answers.append({'text': '', 'answer_start': 0})\r\n","        for ans_idx in range(len(example['answers']['text'])):\r\n","            contexts.append(context)\r\n","            questions.append(question)\r\n","            answer = {'text': example['answers']['text'][ans_idx], 'answer_start': example['answers']['answer_start'][ans_idx]}\r\n","            answers.append(answer)\r\n","    return contexts, questions, answers\r\n","\r\n","train_contexts, train_questions, train_answers = get_info(train_set)\r\n","val_contexts, val_questions, val_answers = get_info(val_set)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Add index where each answer ends\r\n","def add_end_idx(answers, contexts):\r\n","    for answer, context in zip(answers, contexts):\r\n","        gold_text = answer['text']\r\n","        start_idx = answer['answer_start']\r\n","        end_idx = start_idx + len(gold_text)\r\n","\r\n","        # Sometimes squad answers are off by a character or two – fix this\r\n","        if context[start_idx : end_idx] == gold_text:\r\n","            answer['answer_end'] = end_idx\r\n","        elif context[start_idx - 1 : end_idx - 1] == gold_text:\r\n","            answer['answer_start'] = start_idx - 1\r\n","            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\r\n","        elif context[start_idx - 2 : end_idx - 2] == gold_text:\r\n","            answer['answer_start'] = start_idx - 2\r\n","            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\r\n","\r\n","add_end_idx(train_answers, train_contexts)\r\n","add_end_idx(val_answers, val_contexts)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Add token positions to encodings\r\n","def add_token_positions(encodings, answers, tokenizer):\r\n","    start_positions = []\r\n","    end_positions = []\r\n","    for i in range(len(answers)):\r\n","        if answers[i]['answer_end']:\r\n","            start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\r\n","            end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\r\n","        else:\r\n","            start_positions.append(0)\r\n","            end_positions.append(0)\r\n","\r\n","        # If start position is None, the answer passage has been truncated\r\n","        if start_positions[-1] is None:\r\n","            start_positions[-1] = tokenizer.model_max_length\r\n","        if end_positions[-1] is None:\r\n","            end_positions[-1] = tokenizer.model_max_length\r\n","    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\r\n","\r\n","# Tokenize the data\r\n","def tokenize_data(contexts, questions, answers, tokenizer):\r\n","    encodings = tokenizer(contexts, questions, truncation=True, padding='max_length', max_length=384)\r\n","    add_token_positions(encodings, answers, tokenizer)\r\n","    return encodings"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# SQuAD dataset features\r\n","class SquadDataset(Dataset):\r\n","    def __init__(self, contexts, questions, answers, tokenizer):\r\n","        self.contexts = contexts\r\n","        self.questions = questions\r\n","        self.answers = answers\r\n","        self.tokenizer = tokenizer\r\n","\r\n","    def __getitem__(self, idx):\r\n","        encoding = tokenize_data([self.contexts[idx]], [self.questions[idx]], [self.answers[idx]], self.tokenizer)\r\n","        return {key: torch.tensor(val[0]) for key, val in encoding.items()}\r\n","\r\n","    def __len__(self):\r\n","        return len(self.contexts)\r\n","\r\n","# Tokenize datasets\r\n","def prepare_features(tokenizer):\r\n","    train_dataset = SquadDataset(train_contexts, train_questions, train_answers, tokenizer)\r\n","    val_dataset = SquadDataset(val_contexts, val_questions, val_answers, tokenizer)\r\n","    return train_dataset, val_dataset"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Dict collate\r\n","def dict_collate(batch):\r\n","    group_dict = {key: [] for key in batch[0].keys()}\r\n","    for item in batch:\r\n","        for key, val in item.items():\r\n","            if not val.dim():\r\n","                group_dict[key].append(val.unsqueeze(0))\r\n","            else:\r\n","                group_dict[key].append(val)\r\n","    return {key: torch.stack(val) for key, val in group_dict.items()}\r\n","\r\n","# Run training\r\n","def run_training(model, train_set, val_set, args):\r\n","    train_loader = DataLoader(train_set, batch_size=args['batch_size'], shuffle=True, collate_fn=dict_collate)\r\n","    val_loader = DataLoader(val_set, batch_size=args['batch_size'], shuffle=True, collate_fn=dict_collate)\r\n","    optim = AdamW(model.parameters(), lr=args['lr'])\r\n","    history = {\r\n","        'training': {'loss': []},\r\n","        'validation': {'loss': []}\r\n","    }\r\n","    # Train for n_epochs\r\n","    best_loss = float('inf')\r\n","    for epoch in range(1, args['n_epochs'] + 1):\r\n","        train_epoch_loss = run_epoch('train', model, train_loader, optimizer=optim, epoch=epoch, total_epoch=args['n_epochs'])\r\n","        val_epoch_loss = run_epoch('val', model, val_loader, optimizer=optim, epoch=epoch, total_epoch=args['n_epochs'])\r\n","\r\n","        # Save loss/accuracy values for each epoch\r\n","        history['training']['loss'].append(train_epoch_loss)\r\n","        history['validation']['loss'].append(val_epoch_loss)\r\n","\r\n","        # Save model state if needed\r\n","        if val_epoch_loss < best_loss:\r\n","            best_loss = val_epoch_loss\r\n","            torch.save(model.state_dict(), 'squad.pt')\r\n","    return history\r\n","\r\n","# Run a single epoch\r\n","def run_epoch(phase, model, loader, optimizer=None, epoch=0, total_epoch=0):\r\n","    if phase == 'train':\r\n","        model.train()\r\n","    elif phase == 'val':\r\n","        model.eval()\r\n","    agg_loss = 0.0\r\n","    with tqdm(loader, unit='batch', position=0, leave=True) as tepoch:\r\n","        for n_batch, batch in enumerate(tepoch, start=1):\r\n","            if phase == 'train': # Clean gradients on training\r\n","                optimizer.zero_grad()\r\n","                tepoch.set_description(f'Epoch {epoch}/{total_epoch}')\r\n","            elif phase == 'val':\r\n","                tepoch.set_description('Validating')\r\n","\r\n","            # Forward pass\r\n","            input_ids = batch['input_ids'].to(device)\r\n","            attention_mask = batch['attention_mask'].to(device)\r\n","            start_positions = batch['start_positions'].to(device)\r\n","            end_positions = batch['end_positions'].to(device)\r\n","            if phase == 'val':\r\n","                with torch.no_grad():\r\n","                    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\r\n","            else:\r\n","                outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\r\n","            loss = outputs[0]\r\n","            agg_loss += loss.item()\r\n","\r\n","            # Update params\r\n","            if phase == 'train':\r\n","                loss.backward() # Backpropagation only while training\r\n","                optimizer.step() # Update weights only while training\r\n","            current_agg_loss = agg_loss / n_batch\r\n","            tepoch.set_postfix(Loss=current_agg_loss)\r\n","    epoch_loss = float(agg_loss / n_batch)\r\n","    return epoch_loss"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Load pre-trained model\r\n","def get_model(name):\r\n","    name_map = {\r\n","        'bert': ['deepset/bert-base-cased-squad2', 'deepset/bert-base-cased-squad2'],\r\n","        'roberta': ['deepset/roberta-base-squad2', 'deepset/roberta-base-squad2'],\r\n","        'albert': ['twmkn9/albert-base-v2-squad2', 'albert-base-v2'],\r\n","    }\r\n","    model = AutoModelForQuestionAnswering.from_pretrained(name_map[name][0])\r\n","    tokenizer = AutoTokenizer.from_pretrained(name_map[name][1])\r\n","    model.to(device)\r\n","    return model, tokenizer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# ----------------- Metric functions for evaluation ----------------- #\r\n","\r\n","# Normalize text\r\n","def normalize_text(s):\r\n","    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\r\n","    import string, re\r\n","\r\n","    def remove_articles(text):\r\n","        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\r\n","        return re.sub(regex, ' ', text)\r\n","\r\n","    def white_space_fix(text):\r\n","        return ' '.join(text.split())\r\n","\r\n","    def remove_punc(text):\r\n","        exclude = set(string.punctuation)\r\n","        return ''.join(ch for ch in text if ch not in exclude)\r\n","\r\n","    def lower(text):\r\n","        return text.lower()\r\n","\r\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\r\n","\r\n","# Exact match evaluation metric\r\n","def compute_exact_match(prediction, truth):\r\n","    return int(normalize_text(prediction) == normalize_text(truth))\r\n","\r\n","# F1 score evaluation metric\r\n","def compute_f1(prediction, truth):\r\n","    pred_tokens = normalize_text(prediction).split()\r\n","    truth_tokens = normalize_text(truth).split()\r\n","\r\n","    # If either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\r\n","    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\r\n","        return int(pred_tokens == truth_tokens)\r\n","\r\n","    common_tokens = set(pred_tokens) & set(truth_tokens)\r\n","\r\n","    # If there are no common tokens then f1 = 0\r\n","    if len(common_tokens) == 0:\r\n","        return 0\r\n","\r\n","    prec = len(common_tokens) / len(pred_tokens)\r\n","    rec = len(common_tokens) / len(truth_tokens)\r\n","\r\n","    return 2 * (prec * rec) / (prec + rec)\r\n","\r\n","# Retrieve possible answers\r\n","def get_gold_answers(example):\r\n","    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\r\n","\r\n","    gold_answers = [answer for answer in example['answers']['text'] if example['answers']['text']]\r\n","\r\n","    # If gold_answers doesn't exist it's because this is a negative example -\r\n","    # the only correct answer is an empty string\r\n","    if not gold_answers:\r\n","        gold_answers = ['']\r\n","\r\n","    return gold_answers"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Obtain prediction for a specific question & context\r\n","def get_prediction(model, example, tokenizer, nbest=10, null_threshold=1.0):\r\n","    inputs = get_qa_inputs(example, tokenizer).to(device)\r\n","    tokens = to_list(inputs['input_ids'])[0]\r\n","    with torch.no_grad():\r\n","        start_logits, end_logits = model(**inputs).values()  # Forward pass\r\n","\r\n","    # Get sensible preliminary predictions, sorted by score\r\n","    prelim_preds = preliminary_predictions(start_logits, end_logits, inputs['input_ids'], nbest, tokenizer.sep_token_id)\r\n","\r\n","    # Narrow that down to the top nbest predictions\r\n","    nbest_preds = best_predictions(prelim_preds, nbest, tokenizer, tokens, to_list(start_logits)[0], to_list(end_logits)[0])\r\n","\r\n","    # Compute the probability of each prediction\r\n","    probabilities = prediction_probabilities(nbest_preds)\r\n","\r\n","    # Compute score difference\r\n","    score_difference = compute_score_difference(nbest_preds)\r\n","\r\n","    # If score difference > threshold, return the null answer (for questions with no answer)\r\n","    if score_difference > null_threshold:\r\n","        return '', probabilities[-1]\r\n","    else:\r\n","        return nbest_preds[0].text, probabilities[0]\r\n","\r\n","# ----------------- Helper functions for get_prediction ----------------- #\r\n","\r\n","# Load the example, convert to inputs, get tokenized info\r\n","def get_qa_inputs(example, tokenizer):\r\n","    question = example['question']\r\n","    context = example['context']\r\n","    return tokenizer.encode_plus(question, context, return_tensors='pt', truncation=True, max_length=384)\r\n","\r\n","# Clean raw text\r\n","def get_clean_text(tokens, tokenizer):\r\n","    text = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(tokens))\r\n","    text = text.strip()\r\n","    text = ' '.join(text.split())\r\n","    return text\r\n","\r\n","# Calculate probabilities for each prediction\r\n","def prediction_probabilities(predictions):\r\n","    def softmax(x):\r\n","        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\r\n","        e_x = np.exp(x - np.max(x))\r\n","        return e_x / e_x.sum()\r\n","\r\n","    all_scores = [pred.start_logit + pred.end_logit for pred in predictions]\r\n","    return softmax(np.array(all_scores))\r\n","\r\n","# Convert tensor to list\r\n","def to_list(tensor):\r\n","    return tensor.detach().cpu().tolist()\r\n","\r\n","# Get preliminary predictions\r\n","def preliminary_predictions(start_logits, end_logits, input_ids, nbest, sep_token_id):\r\n","    # Convert tensors to lists\r\n","    start_logits = to_list(start_logits)[0]\r\n","    end_logits = to_list(end_logits)[0]\r\n","    tokens = to_list(input_ids)[0]\r\n","\r\n","    # Sort our start and end logits from largest to smallest, keeping track of the index\r\n","    start_idx_and_logit = sorted(enumerate(start_logits), key=lambda x: x[1], reverse=True)\r\n","    end_idx_and_logit = sorted(enumerate(end_logits), key=lambda x: x[1], reverse=True)\r\n","    start_indexes = [idx for idx, logit in start_idx_and_logit[:nbest]]\r\n","    end_indexes = [idx for idx, logit in end_idx_and_logit[:nbest]]\r\n","\r\n","    # Question tokens are between the CLS token (101, at position 0) and first SEP (102) token\r\n","    question_indexes = [i + 1 for i, token in enumerate(tokens[1 : tokens.index(sep_token_id)])]\r\n","\r\n","    # Keep track of all preliminary predictions\r\n","    PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\r\n","        'PrelimPrediction', ['start_index', 'end_index', 'start_logit', 'end_logit']\r\n","    )\r\n","    prelim_preds = []\r\n","    for start_index in start_indexes:\r\n","        for end_index in end_indexes:\r\n","            # Throw out invalid predictions\r\n","            if start_index in question_indexes:\r\n","                continue\r\n","            if end_index in question_indexes:\r\n","                continue\r\n","            if end_index < start_index:\r\n","                continue\r\n","            prelim_preds.append(\r\n","                PrelimPrediction(\r\n","                    start_index=start_index,\r\n","                    end_index=end_index,\r\n","                    start_logit=start_logits[start_index],\r\n","                    end_logit=end_logits[end_index]\r\n","                )\r\n","            )\r\n","    # Sort prelim_preds in descending score order\r\n","    prelim_preds = sorted(prelim_preds, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\r\n","    return prelim_preds\r\n","\r\n","# Filter the nbest predictions\r\n","def best_predictions(prelim_preds, nbest, tokenizer, tokens, start_logits, end_logits):\r\n","    # This will be the pool from which answer probabilities are computed\r\n","    BestPrediction = collections.namedtuple(\r\n","        'BestPrediction', ['text', 'start_logit', 'end_logit']\r\n","    )\r\n","    nbest_predictions = []\r\n","    seen_predictions = []\r\n","    for pred in prelim_preds:\r\n","        if len(nbest_predictions) >= nbest:\r\n","            break\r\n","        if pred.start_index > 0: # Non-null answers\r\n","            toks = tokens[pred.start_index : pred.end_index + 1]\r\n","            text = get_clean_text(toks, tokenizer)\r\n","\r\n","            # If this text has been seen already - skip it\r\n","            if text in seen_predictions:\r\n","                continue\r\n","\r\n","            # Flag text as being seen\r\n","            seen_predictions.append(text)\r\n","\r\n","            # Add this text to a pruned list of the top nbest predictions\r\n","            nbest_predictions.append(\r\n","                BestPrediction(\r\n","                    text=text,\r\n","                    start_logit=pred.start_logit,\r\n","                    end_logit=pred.end_logit\r\n","                )\r\n","            )\r\n","\r\n","    # Add the null prediction\r\n","    nbest_predictions.append(\r\n","        BestPrediction(\r\n","            text='',\r\n","            start_logit=start_logits[0],\r\n","            end_logit=end_logits[0]\r\n","        )\r\n","    )\r\n","    return nbest_predictions\r\n","\r\n","# Calculate score to check if answer should be null\r\n","def compute_score_difference(predictions):\r\n","    \"\"\" Assumes that the null answer is always the last prediction \"\"\"\r\n","    score_null = predictions[-1].start_logit + predictions[-1].end_logit\r\n","    score_non_null = predictions[0].start_logit + predictions[0].end_logit\r\n","    return score_null - score_non_null"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# ----------------- Evaluation ----------------- #\r\n","\r\n","# Evaluate a single example\r\n","def evaluate(model, example, tokenizer, nbest=10, null_threshold=-3.767639636993408):\r\n","    model.eval()\r\n","    prediction = get_prediction(model, example, tokenizer, nbest=nbest, null_threshold=null_threshold)\r\n","    gold_answers = get_gold_answers(example)\r\n","    em_score = max((compute_exact_match(prediction[0], answer)) for answer in gold_answers)\r\n","    f1_score = max((compute_f1(prediction[0], answer)) for answer in gold_answers)\r\n","    print(f'Context: {example[\"context\"]}\\n')\r\n","    print(f'Question: {example[\"question\"]}')\r\n","    print(f'Prediction: {prediction[0] if prediction[0] else \"NO ANSWER\"}')\r\n","    print(f'True Answers: {gold_answers}')\r\n","    print(f'EM: {em_score} \\t F1: {f1_score}')\r\n","\r\n","# Evaluate on the SQuAD dev set\r\n","def run_testing(model, examples, tokenizer, nbest=10, null_threshold=-3.767639636993408):\r\n","    model.eval()\r\n","    em_score_total = 0\r\n","    f1_score_total = 0\r\n","    for example in examples:\r\n","        prediction = get_prediction(model, example, tokenizer, nbest=nbest, null_threshold=null_threshold)\r\n","        gold_answers = get_gold_answers(example)\r\n","        em_score_total += max((compute_exact_match(prediction[0], answer)) for answer in gold_answers)\r\n","        f1_score_total += max((compute_f1(prediction[0], answer)) for answer in gold_answers)\r\n","    em_score_avg = round(100 * (em_score_total / len(examples)), 2)\r\n","    f1_score_avg = round(100 * (f1_score_total / len(examples)), 2)\r\n","    print(f'Avg EM: {em_score_avg}% \\t Avg F1: {f1_score_avg}%')"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# RoBERTa model\r\n","roberta_model, roberta_tokenizer = get_model('roberta')\r\n","\r\n","# ALBERT model\r\n","albert_model, albert_tokenizer = get_model('albert')"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Example evaluation\r\n","example = test_set[0]\r\n","evaluate(roberta_model, example, roberta_tokenizer)\r\n","evaluate(albert_model, example, albert_tokenizer)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Run testing with both models over SQuAD dev set\r\n","run_testing(roberta_model, test_set, roberta_tokenizer)\r\n","run_testing(albert_model, test_set, albert_tokenizer)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Ensemble\r\n","class BertEnsamble(PreTrainedModel):\r\n","    def __init__(self, config, *args, **kwargs):\r\n","      super().__init__(config)\r\n","\r\n","      # BertModel\r\n","      self.bert_model_1 = BertModel(config['BertConfig'])\r\n","\r\n","      # Model 2\r\n","      self.bert_model_2 = BertModel(config['BertConfig'])\r\n","\r\n","      # FC\r\n","      self.cls = nn.Linear(config['BertConfig'].hidden_size, 2)\r\n","\r\n","    def forward(\r\n","      self,\r\n","      input_ids=None,\r\n","      attention_mask=None,\r\n","      token_type_ids=None,\r\n","      position_ids=None,\r\n","      head_mask=None,\r\n","      inputs_embeds=None,\r\n","      next_sentence_label=None,\r\n","    ):\r\n","      outputs = []\r\n","      input_ids_1 = input_ids[0]\r\n","      attention_mask_1 = attention_mask[0]\r\n","      outputs.append(self.bert_model_1(input_ids_1,\r\n","                                      attention_mask=attention_mask_1))\r\n","\r\n","      input_ids_2 = input_ids[1]\r\n","      attention_mask_2 = attention_mask[1]\r\n","      outputs.append(self.bert_model_2(input_ids_2,\r\n","                                      attention_mask=attention_mask_2))\r\n","\r\n","      # just get the [CLS] embeddings\r\n","      last_hidden_states = torch.cat([output[1] for output in outputs], dim=1)\r\n","      logits = self.cls(last_hidden_states)\r\n","\r\n","      # crossentropyloss\r\n","      if next_sentence_label is not None:\r\n","        loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\r\n","        next_sentence_loss = loss_fct(logits.view(-1, 2), next_sentence_label.view(-1))\r\n","        return next_sentence_loss, logits\r\n","      else:\r\n","        return logits"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["config = {\r\n","    'BertConfig': BertConfig(),\r\n","    'RobertaConfig': RobertaConfig(),\r\n","}"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["training_args = {\r\n","    'batch_size': 8,\r\n","    'lr': 3e-5,\r\n","    'n_epochs': 1,\r\n","}\r\n","train_dataset, val_dataset = prepare_features(albert_tokenizer)\r\n","loss_history = run_training(albert_model, train_dataset, val_dataset, training_args)"],"outputs":[],"metadata":{}}]}